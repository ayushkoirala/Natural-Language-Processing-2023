
#Paper : A Fast and Accurate Dependency Parser using Neural Network
#Link : https://aclanthology.org/D14-1082.pdf

|   Topic               |  A Fast and Accurate Dependency Parser using Neural Network   |
| --------              |  ----------------------------                                 |
| Problem Statement     | The problem with current dependency parsers is that they classify based on millions of sparse indicator features, which results in poor generalization and slow parsing speed due to the high cost of feature computation. The need for a solution is to find a more efficient and effective way of classifying in a greedy, transition-based dependency parser. |
|   Key Related work    | 1. Kubler et al. (2009) proposed the use of feature-based discriminative dependency parsers, which achieved enormous parsing success in recent years. <br /> 2. Koo et al. (2008) introduced techniques for higher-support features such as word class features to improve parsing performance. <br /> 3. He et al. (2013) identified the problem of high computational cost in feature extraction in modern dependency parsers.<br /> 4. Bohnet (2010) reported that feature extraction consumed 99% of the runtime in his baseline parser. <br />5. Collobert et al. (2011) demonstrated the effectiveness of distributed word representations in NLP tasks such as POS tagging.<br />6. Devlin et al. (2014) showed the usefulness of distributed word representations in machine translation.<br />7. Socher et al. (2013) used low-dimensional, dense word embeddings to improve constituency parsing.<br /> |
| Solution | The solution proposed in the paper is a novel way of using a neural network classifier in a greedy transition-based dependency parser to address the problems faced by the current feature-based parsers.|
| Methodology | 1. Representing words, POS tags, and arc labels as vectors: Each word is represented as a d-dimensional vector, while POS tags and arc labels are also mapped to a d-dimensional vector space. <br /> 2. Selecting elements based on stack/buffer positions: A set of elements is chosen based on the stack/buffer positions for each type of information (word, POS, or label). <br /> 3. Building a standard neural network with one hidden layer: The input layer contains the corresponding embeddings of the chosen elements from the sets of words, POS tags, and arc labels. The input layer is then mapped to a hidden layer with dh nodes through a cube activation function. <br /> 4. Adding a softmax layer: A softmax layer is added on top of the hidden layer for modeling multi-class probabilities. |
| Results | 1. The comparison of accuracy and parsing speed of the new parser with greedy arc-eager and arc-standard parsers and two popular off-the-shelf parsers (MaltParser and MSTParser) was performed on PTB (CoNLL dependencies), PTB (Stanford dependencies) and CTB datasets. <br />2.The results showed that the new parser outperforms the other parsers in terms of both accuracy and speed, achieving around 2% improvement in UAS and LAS on all datasets, and running about 20 times faster than the baseline arc-eager and arc-standard parsers. It also surpasses the efficiency of MaltParser with liblinear, and is nearly 100 times faster than MSTParser, especially for the fine-grained label set of Stanford dependencies. |
| Conclusion |The novel dependency parser presented in the study uses neural networks and outperforms other greedy parsers in terms of accuracy and speed. This is achieved by using dense word, POS tag, and arc label vectors and modeling their interactions with a unique cube activation function. The model relies solely on dense features, allowing for automatic learning of the most useful feature combinations for predictions. |
