
#Paper : A Fast and Accurate Dependency Parser using Neural Network
#Link : https://aclanthology.org/D14-1082.pdf

|   Topic               |  A Fast and Accurate Dependency Parser using Neural Network   |
| --------              |  ----------------------------                                 |
| Problem Statement     | The problem with current dependency parsers is that they classify based on millions of sparse indicator features, which results in poor generalization and slow parsing speed due to the high cost of feature computation. The need for a solution is to find a more efficient and effective way of classifying in a greedy, transition-based dependency parser. |
|   Key Related work    | 1. Kubler et al. (2009) proposed the use of feature-based discriminative dependency parsers, which achieved enormous parsing success in recent years. <br /> 2. Koo et al. (2008) introduced techniques for higher-support features such as word class features to improve parsing performance. <br /> 3. He et al. (2013) identified the problem of high computational cost in feature extraction in modern dependency parsers.<br /> 4. Bohnet (2010) reported that feature extraction consumed 99% of the runtime in his baseline parser. <br />5. Collobert et al. (2011) demonstrated the effectiveness of distributed word representations in NLP tasks such as POS tagging.<br />6. Devlin et al. (2014) showed the usefulness of distributed word representations in machine translation.<br />7. Socher et al. (2013) used low-dimensional, dense word embeddings to improve constituency parsing.<br /> |
| Solution | The solution proposed in the paper is a novel way of using a neural network classifier in a greedy transition-based dependency parser to address the problems faced by the current feature-based parsers.|
| Methodology | 1. Representing words, POS tags, and arc labels as vectors: Each word is represented as a d-dimensional vector, while POS tags and arc labels are also mapped to a d-dimensional vector space. <br /> 2. Selecting elements based on stack/buffer positions: A set of elements is chosen based on the stack/buffer positions for each type of information (word, POS, or label). <br /> 3. Building a standard neural network with one hidden layer: The input layer contains the corresponding embeddings of the chosen elements from the sets of words, POS tags, and arc labels. The input layer is then mapped to a hidden layer with dh nodes through a cube activation function. <br /> 4. Adding a softmax layer: A softmax layer is added on top of the hidden layer for modeling multi-class probabilities. |
