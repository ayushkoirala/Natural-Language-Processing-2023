This repository is for submitting Paper redign assignment  for AT82.05 Artificial Intelligence:NLU at Asian Institute of Technology by chaklam silpasuwanchai

## Paper reading assignment- 1

## 1.  Improving Text Auto-Completion with Next Phrase Prediction :  https://aclanthology.org/2021.findings-emnlp.378.pdf

| Topic  | Improving Text Auto-Completion with Next Phrase Prediction |
|--------------|--------------------------------------------------------------------------------------------------------|
| Hypothesis Question | Does the proposed intermediate training strategy improve the adaptability and performance of pre-trained language models like GPT-2 in text auto-completion for specific domains(i.e Computer Science, medical)? |
| Key Related Work | 1. "Evaluation of Deep Neural Models for Text Auto-Completion Performance": This work could compare the performance of different deep neural models, including the ones mentioned in the original text, on the text auto-completion task. It could explore factors such as accuracy, efficiency, and scalability of these models, and provide insights on their strengths and weaknesses for this task. <br /> 2. "Challenges in Adapting Deep Neural Models for Text Auto-Completion in Specific Domains: A Case Study of Academic Writing": This work could focus on the difficulties of adapting these models to specific domains, such as academic writing, as mentioned in the original text. It could examine the impact of domain-specific training corpus size, domain-specific language patterns, and domain-specific writing styles on the performance of these models. Additionally, this work could propose solutions to overcome these challenges and enhance the adaptability of these models to specific domains."  |
| Solution | Exploiting the Decoder-only Transformer model such as GPT-2 can improve text auto-completion performance, but requires a large fine-tuning effort for domain-specific expert sentences. |
| Method |1. Phrase extraction through constituency parsing: The first step of the proposed intermediate training strategy involves extracting qualitative phrases from the text data using a constituency parsing method. This process helps the model understand the complete phrase structure, and not just a fraction of the sentence, for the next step of the process. <br /> 2. Generative Question Answering through next phrase prediction: The second step is called Next Phrase Prediction (NPP), a self-supervised objective that guides the pre-trained language model to choose the correct next phrase from the set of possible phrases in the sentence. This process helps the model understand the context of the text and make more informed suggestions during text auto-completion.<br /> 3. Guiding the pre-trained language model to choose the correct next phrase: During the NPP step, the pre-trained language model is guided to choose the correct next phrase among other phrases of the same type (e.g., noun phrase, verb phrase, etc.) in the sentence. This helps the model understand the relationship between different phrases in a sentence and make more accurate predictions during text auto-completion. <br /> 4. Using self-supervised objective called Next Phrase Prediction (NPP) as an intermediate training strategy: The overall intermediate training strategy proposed in this paper is based on the NPP self-supervised objective, which incrementally trains the pre-trained language model to provide better auto-completion suggestions and adapt to specific domains faster during fine-tuning. The authors claim that this is the first work to propose such an intermediate training strategy for improving language models' performance on the text auto-completion task.|
| Result |There are 2 datasets: 1. Emails 2. Academic Writing
Evualtion matrics are : BLEU-4, METEOP,CIDEr, SPICE
For academic writing : NPP + T5 model performs better than NSP +T5, T5 and GPT-2
for Emails writing : NPP + T5 model performs better than NSP +T5, T5 and GPT-2 |

| Conclusion | The proposed intermediate training strategy showed improved performance in the text auto-completion task for email and academic-writing domains with limited training data, outperforming existing baselines.|

